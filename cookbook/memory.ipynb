{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4832ef5b",
   "metadata": {},
   "source": [
    "# Add Memory to the `Arborist`\n",
    "\n",
    "For more conversational applications it is possible to pass chat history to the `Arborist`. In this cookbook we simulate a trivial conversation in order to illustrate the use of `Memory`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e6e18",
   "metadata": {},
   "source": [
    "## Memory implementation\n",
    "\n",
    "For our simulated conversation we implement a super simple version of the `Memory` abstraction which simply keeps an in-memory list of `ChatMessage`s. A real client would typically wrap a more powerful memory strategy (e.g. `llamaindex` long-short memory implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20509c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from treelang.ai.memory import Memory, ChatMessage\n",
    "\n",
    "class SimpleMemory(Memory):\n",
    "    \"\"\"A simple in-memory implementation of the Memory abstraction.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.messages: List[ChatMessage] = []\n",
    "\n",
    "    async def add(self, messages: List[ChatMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    async def get(self) -> List[ChatMessage]:\n",
    "        return self.messages\n",
    "\n",
    "    async def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ae3fb",
   "metadata": {},
   "source": [
    "## The chat\n",
    "\n",
    "For this super simple chat we re-use our trusty `MCP` calculator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f73ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "# mcp components for stdio communication\n",
    "from mcp import ClientSession, StdioServerParameters, stdio_client\n",
    "\n",
    "# our helpful `Arborist` implementation\n",
    "from treelang.ai.arborist import OpenAIArborist\n",
    "from treelang.ai.provider import MCPToolProvider\n",
    "\n",
    "async def main():\n",
    "    # server parameters for running the calculator server\n",
    "    path = os.path.join(os.getcwd(), \"calculator.py\")\n",
    "    memory = SimpleMemory()\n",
    "    server_params = StdioServerParameters(\n",
    "        path=path, \n",
    "        command=\"python\", args=[path], env=None\n",
    "    )\n",
    "    \n",
    "    # establish a communication channel with the server\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        # create a client session for interaction\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # initialize the session\n",
    "            await session.initialize()\n",
    "            provider = MCPToolProvider(session)\n",
    "            # create and configure the Arborist\n",
    "            arborist = OpenAIArborist(\n",
    "                model=\"gpt-4o-2024-11-20\", \n",
    "                provider=provider, \n",
    "                memory=memory\n",
    "            )\n",
    "            # simulate a conversation\n",
    "            query1 = \"Add 2 and 3.\"\n",
    "            response1 = await arborist.eval(query1)\n",
    "            print(\"First response:\", response1.content)\n",
    "\n",
    "            # store the first interaction in memory\n",
    "            await memory.add([\n",
    "                ChatMessage(role=\"user\", content=query1),\n",
    "                ChatMessage(role=\"assistant\", content=str(response1.content))\n",
    "            ])\n",
    "\n",
    "            # now ask a follow-up question!\n",
    "            query2 = \"Now multiply the result by 4.\"\n",
    "            response2 = await arborist.eval(query2)\n",
    "            print(\"Second response:\", response2.content)\n",
    "\n",
    "            # store the second interaction in memory\n",
    "            await memory.add([\n",
    "                ChatMessage(role=\"user\", content=query2),\n",
    "                ChatMessage(role=\"assistant\", content=str(response2.content))\n",
    "            ])\n",
    "\n",
    "            query3 = \"Instead of adding the first two numbers, what if we subtracted them?\"\n",
    "            response3 = await arborist.eval(query3)\n",
    "            \n",
    "            print(\"Third response:\", response3.content)\n",
    "\n",
    "            await memory.add([\n",
    "                ChatMessage(role=\"user\", content=query3),\n",
    "                ChatMessage(role=\"assistant\", content=str(response3.content))\n",
    "            ])\n",
    "\n",
    "            # we're done! Let's see the conversation history\n",
    "            print(\"Conversation history:\")\n",
    "\n",
    "            for msg in await memory.get():\n",
    "                print(f\"{msg.role}: {msg.content}\") \n",
    "            \n",
    "            # finally, clear the memory\n",
    "            await memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60681585",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70bc505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task pending name='Task-5' coro=<main() running at /tmp/ipykernel_28629/3785840027.py:11>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First response: 5.0\n",
      "Second response: 20.0\n",
      "Third response: -4.0\n",
      "Conversation history:\n",
      "user: Add 2 and 3.\n",
      "assistant: 5.0\n",
      "user: Now multiply the result by 4.\n",
      "assistant: 20.0\n",
      "user: Instead of adding the first two numbers, what if we subtracted them?\n",
      "assistant: -4.0\n"
     ]
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.create_task(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
